{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Taxis Fare Prediction Analysis Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors: Han Wang, Jam Lin, Jiayi Li, Yibin Long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This report was developed as a deliverable for the term project in DSCI 522 (Data Science Workflows), a course in the Master of Data Science program at the University of British Columbia.\n",
    "\n",
    "The overall objective of this project was to automate a typical data science workflow. This report summarizes the results of a series of automated Python scripts that handle tasks such as data retrieval, data cleaning, exploratory data analysis (EDA), creation of a predictive machine learning model, and interpretation of the results. The report provides a detailed explanation of each step, applying it to the specific context of the dataset in question. It assumes the reader has a basic understanding of machine learning terminology and concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This report presents a linear regression model developed to predict NYC taxi fare amount. Using data from 30,000 taxi trips in January 2024, we build a linear regression model using trip distance feature, with each additional mile increasing the fare by approximately $3.54. The model was evaluated using a test dataset, where predicted fare amounts were compared to actual fares. While the model performed well overall, some outliers were identified, suggesting the need for further data cleaning and additional features to improve accuracy. Future steps include incorporating more features, experimenting with other regression models like KNN and Lasso regression, and addressing hyperparameters for better generalizability and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction/Background\n",
    "\n",
    "Taking a taxi in New York City can be overwhelming for first-time visitors, especially for tourists unfamiliar with the city's layout and fare system. With over 200,000 taxi trips taking place daily, yellow cabs remain an essential mode of transportation for both locals and visitors. However, the lack of transparency often leads to concerns among tourists about overcharging or being taken on unnecessarily long routes.\n",
    "\n",
    "To address these concerns, we use data from 30,000 Yellow Taxi trips recorded in January 2024, provided by the NYC Taxi and Limousine Commission (TLC). Our analysis examines how to predict taxi fare amounts, offering valuable insights to help tourists better understand what they should expect to pay for a taxi ride in NYC. By leveraging data-driven analysis, we aim to provide a clearer and more predictable fare structure, enabling tourists to make more informed decisions during their travels in the city."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis Question:\n",
    "How do we best predict NYC yellow taxi fare amounts?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set used in this project is the TLC Trip Record Data, which includes taxi and for-hire vehicle trip records collected by technology providers authorized under the Taxicab & Livery Passenger Enhancement Programs (TPEP/LPEP). This data is sourced from the NYC Taxi and Limousine Commission (TLC) and can be accessed from the NYC Taxi and Limousine Commission's [website](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page).\n",
    "Yellow and green taxi trip records contain fields capturing pick-up and drop-off dates and times, pick-up and drop-off locations, trip distances, itemized fares, rate types, payment types, and driver-reported passenger counts. Similarly, For-Hire Vehicle (FHV) trip records include fields such as dispatching base license number, pick-up date and time, and taxi zone location ID.\n",
    "It is important to note that the TLC did not create this data and makes no representations regarding its accuracy. FHV trip records are based on submissions from dispatching bases and may not represent the total volume of trips. Additionally, the TLC reviews these records and enforces necessary actions to improve their accuracy and completeness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods and Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to address our research question, we will begin by selecting the appropriate features from the dataset through exploratory data analysis (EDA) and by consulting the data dictionary to better understand the instances within the dataset (dataset information is linked in the references). Since this is a linear regression modeling problem, we will use the LinearRegression model as our model of choice. Linear regression is a fundamental and widely used method for predictive modeling.\n",
    "The dataset used in this study includes daily fare amounts and trip distance data from January 2024, covering 30,000 observations. The data will be split into training and test sets, with 70% allocated for training and 30% for testing.\n",
    "\n",
    "The analysis will be conducted using the Python programming language [Van Rossum and Drake, 2009], with the following Python packages: numpy [Harris et al., 2020], pandas [McKinney, 2010; VanderPlas, 2018], scikit-learn [Pedregosa et al., 2011], and altair [VanderPlas et al., 2018].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import altair as alt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "data_set_link = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet\"\n",
    "\n",
    "# Use only a smaller, random subset (30,000 rows) of the data\n",
    "df = pd.read_parquet(data_set_link).sample(30000, random_state=123)\n",
    "df.to_csv('data/yellow_tripdata_2024-01.csv', index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets do some data validation by checking the target variable fare_amount's distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \"vegafusion[embed]>=1.5.0\"\n",
    "\n",
    "alt.data_transformers.enable(\"vegafusion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = alt.Chart(df).transform_density(\n",
    "    'fare_amount',\n",
    "    as_=['fare_amount', 'density']\n",
    ").mark_area().encode(\n",
    "    x=alt.X('fare_amount', title='Fare Amount'),\n",
    "    y=alt.Y('density:Q', title='Density'),\n",
    ").properties(\n",
    "    title='Fare Amount Density Chart',\n",
    "    width=600,\n",
    "    height=400\n",
    ")\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this chart, we see that most values of the fare amount make sense - they are mostly under $50, with a cluster of fare amounts at around 65 USD, likely a longer trip from the airport. However, we see fare amounts that are lower that 0 which are not explained by the [documentation](https://www.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf), that \"The time-and-distance fare calculated by the meter.\" They might be simply refunded trips, but since we cannot confirm, we will choose to drop rows where the fare amount is negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['fare_amount'] >= 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll split the dataset into training set and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.3, random_state=123)\n",
    "X_train, y_train = train_df['trip_distance'].values.reshape(-1,1), train_df['fare_amount'].values\n",
    "X_test, y_test = test_df['trip_distance'].values.reshape(-1,1), test_df['fare_amount'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll perform a summary of the data set that is relevant for exploratory data analysis related to our regression analysis. We'll check out the summary statistics for each column in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the summary statistics, for the fare amount, the mean fare amount is 18.59 USD, while the median is 12.8 USD, which again points to a right-skewed distribution for this column. This confirms what we saw with the density chart. We also note that not all the counts of the columns are the same, perhaps there are some missing values!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll create a visualizations for exploratory data analysis. First, we want to confirm where in the dataset there are missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing values\n",
    "\n",
    "alt.Chart(\n",
    "    train_df.isna().reset_index().melt(\n",
    "        id_vars='index'\n",
    "    )\n",
    ").mark_rect().encode(\n",
    "    alt.X('index:O').axis(None),\n",
    "    alt.Y('variable').title(None),\n",
    "    alt.Color('value').title('NaN'),\n",
    "    alt.Stroke('value')\n",
    ").properties(\n",
    "    width=df.shape[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As evidenced by the chart, the only columns that contain null values are Airport_fee, RatecodeID, congestion_surcharge, passenger_count, and store_and_fwd_flag. We may want to drop the rows with null values, but it will only matter if we actually use the columns that contain null values in the first place. So, lets instead check the correlation plot to see if we want to include any of these columns with null values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create a correlation plot of all of the columns against one another, to check out the strength and direction of associations between columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Plot\n",
    "\n",
    "corr_df = train_df.select_dtypes('number').corr('spearman', numeric_only=True).stack().reset_index(name='corr')\n",
    "corr_df.loc[corr_df['corr'] == 1, 'corr'] = 0  # Remove diagonal\n",
    "corr_df['abs'] = corr_df['corr'].abs()\n",
    "\n",
    "alt.Chart(corr_df).mark_circle().encode(\n",
    "    x='level_0',\n",
    "    y='level_1',\n",
    "    size=alt.Size('abs').scale(domain=(0, 1)),\n",
    "    color=alt.Color('corr').scale(scheme='redblue', domain=(-1, 1))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the correlation plot, we see that \"trip_distance\" and \"fare_amount\" have a fairly high positive correlation, which may indicate that they are fairly positively associated with each other. We decide now to use \"trip_distance\" to try and predict \"fare_amount\". Since there are no null values in \"trip_distance\", no null values need to be dropped for \"trip_distance\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll do a data validation check that the missingness is not beyond the expected threshold value. We'll set the threshold of missing values to 5%. Since we already checked that the columns we care about, \"trip_distance\" and \"fare_amount\" do not have any missing values, we pass these checks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have chosen the target variable and the explanatory variable, we'll check the schema is valid for both of these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandera as pa\n",
    "from src.schema_postEDA import get_taxi_postEDA_data_schema\n",
    "\n",
    "schema = get_taxi_postEDA_data_schema()\n",
    "\n",
    "try:\n",
    "    schema.validate(train_df[['trip_distance', \"fare_amount\"]])\n",
    "    print(\"Successfully validated the post EDA schema!\")\n",
    "except pa.errors.SchemaError as e:\n",
    "    print(f\"Invalid data failed validation as expected: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in the Methods summary, we will now build and test our Linear Regression Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-learn\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "intercept = model.intercept_\n",
    "slope = model.coef_[0]\n",
    "\n",
    "test_predictions = pd.DataFrame({\n",
    "    'trip_distance': X_test.flatten(),\n",
    "    'fare_amount': y_test,\n",
    "    'y_pred': model.predict(X_test)\n",
    "})\n",
    "\n",
    "print(f\"The regression line formula is: y_hat = {slope:.4f} * trip_distance + {intercept:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's calculate some error metrics and perform a visualization of the result of the regression model in the form of a scatter plot with the regression line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate error metrics\n",
    "y_true = test_predictions['fare_amount']\n",
    "y_pred = test_predictions['y_pred']\n",
    "test_predictions['residuals'] = y_true - y_pred\n",
    "test_predictions['abs_residuals'] = np.abs(test_predictions['residuals'])\n",
    "\n",
    "metrics = {\n",
    "    'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "    'R²': r2_score(y_true, y_pred),\n",
    "    'MAE': mean_absolute_error(y_true, y_pred)\n",
    "}\n",
    "\n",
    "print(\"\\nRegression Performance Metrics:\")\n",
    "print(f\"RMSE: ${metrics['RMSE']:.2f}\")\n",
    "print(f\"R²: {metrics['R²']:.3f}\")\n",
    "print(f\"MAE: ${metrics['MAE']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_scatter = alt.Chart(test_predictions).mark_circle(size=60, opacity=0.3).encode(\n",
    "    x=alt.X('fare_amount', title='Actual Fare Amount ($)'),\n",
    "    y=alt.Y('y_pred', title='Predicted Fare Amount ($)'),\n",
    "    tooltip=['trip_distance', 'fare_amount', 'y_pred', 'residuals']\n",
    ").properties(\n",
    "    width=400,\n",
    "    height=400,\n",
    "    title='Predicted vs Actual Taxi Fare Amounts With Error Line'\n",
    ")\n",
    "error_diagonal = alt.Chart(pd.DataFrame({\n",
    "    'x': [test_predictions['fare_amount'].min(), test_predictions['fare_amount'].max()]\n",
    "})).mark_line(color='red', strokeDash=[4, 4]).encode(\n",
    "    x='x',\n",
    "    y='x'\n",
    ")\n",
    "\n",
    "error_scatter + error_diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_plot = alt.Chart(test_predictions).mark_circle().encode(\n",
    "    x=alt.X('trip_distance', title='Trip Distance (miles)'),\n",
    "    y=alt.Y('fare_amount', title=\"Fare Amount (USD)\"),\n",
    "    color=alt.value('purple'),\n",
    "    tooltip=['trip_distance', 'fare_amount']\n",
    ").properties(\n",
    "    title=\"Regression of Trip Distance vs Fare Amount for NYC Yellow Taxis in January 2024\"\n",
    ")\n",
    "\n",
    "line_plot = alt.Chart(test_predictions).mark_line(color='orange').encode(\n",
    "    x='trip_distance',\n",
    "    y='y_pred'\n",
    ")\n",
    "\n",
    "chart = scatter_plot + line_plot\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of the analysis was to predict the fare amount using a linear regression model. The regression plot, generated from the model, demonstrates a positive linear relationship between trip distance and fare amount for NYC Yellow Taxis in January 2024. This confirms that as the trip distance increases, the fare amount also increases, as expected.\n",
    "\n",
    "The linear regression model shows moderate predictive power with an R-squared value of 0.848, which explains about 84.8% percent of fare variance. The low MAE of 3.24 USD indicates good accuracy for typical rides, although the higher RMSE (6.78 USD) suggests sensitivity to outliers. The visual analysis of the errors reveals the model performs best for the 10-50 USD range, and underestimates fares over 100 USD. The actual NYC taxi fare is calculated through initial fare, per-mile charge, and additional fees for peak hours, nighttime, tolls, etc that this model cannot predict, but can be built further using more complicated multilinear regression or ML models.\n",
    "\n",
    "Based on the estimated result from the regression line formula derived from the model, this suggests that for each additional mile traveled, the fare increases by approximately 3.62 USD, and the base fare (when the trip distance is zero) is around 7.02 USD.\n",
    "\n",
    "To evaluate the model's predictive performance, we applied it to the test data, which was split from the original dataset (70 percent training, 30 percent testing). The predicted fare amounts were compared against the actual fare amounts in the test set. The predicted values were calculated using the formula derived from the model, where trip distances from the test set were used as input to generate the corresponding fare predictions.\n",
    "\n",
    "A scatter plot of the actual fare amounts versus trip distances, along with the regression line, was generated to visualize the results. The plot showed that the model fits most of the data well, but there were some outliers where the predicted fare deviated significantly from the actual values. These discrepancies could be due to errors or special cases in the fare data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limitations and Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, our model may be useful in the initial analysis of tourists interested in making informal predictions about NYC taxi fare amounts. However, there are several areas where this work can be improved. In this analysis, we did not include hyperparameters like the regularization parameter (α), which is essential for controlling overfitting and improving model generalizability. Furthermore, the model currently uses only a single feature (trip distance), which may result in underfitting and bias. In the next steps, we plan to incorporate more features into the model and refine the approach to improve its precision and predictive power. Additionally, experimenting with other regression models, such as KNN regression for non-linear relationships and Lasso regression for linear relationships, could be valuable to see if these models improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Charles R Harris, K Jarrod Millman, Stéfan J van der Walt, Ralf Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J Smith, Robert Kern, Matti Picus, Stephan Hoyer, Marten H van Kerkwijk, Matthew Brett, Allan Haldane, Jaime Fernández del Río, Mark Wiebe, Pearu Peterson, Pierre Gérard-Marchant, Kevin Sheppard, Tyler Reddy, Warren Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis E Oliphant. Array programming with NumPy. Nature, 585(7825):357–362, 2020. URL: https://doi.org/10.1038/s41586-020-2649-2, doi:10.1038/s41586-020-2649-2.\n",
    "\n",
    "F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011.\n",
    "\n",
    "Jake VanderPlas. Altair: interactive statistical visualizations for python. Journal of open source software, 3(7825):1057, 2018. URL: https://doi.org/10.21105/joss.01057, doi:10.21105/joss.01057.\n",
    "\n",
    "Guido Van Rossum and Fred L. Drake. Python 3 Reference Manual. CreateSpace, Scotts Valley, CA, 2009. ISBN 1441412697.\n",
    "\n",
    "New York City Taxi and Limousine Commission. TLC Trip Record Data. Retrieved from https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page, 2024.\n",
    "\n",
    "Wes McKinney. Data structures for statistical computing in python. In Stéfan van der Walt and Jarrod Millman, editors, Proceedings of the 9th Python in Science Conference, =51 – 56. 2010.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !jupyter nbconvert --to webpdf yellow_taxi_analysis.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:573]",
   "language": "python",
   "name": "conda-env-573-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
